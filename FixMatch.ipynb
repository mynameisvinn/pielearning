{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "%matplotlib inline\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(k, b):\n",
    "    \"\"\"return dataloader corresponding to target k.\n",
    "    \"\"\"\n",
    "\n",
    "    # fetch dataset\n",
    "    dataset = datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "    # select appropriate indices\n",
    "    idx = dataset.train_labels==k\n",
    "\n",
    "    # subset raw dataset by indices\n",
    "    dset_train = torch.utils.data.dataset.Subset(dataset, np.where(idx==True)[0])\n",
    "\n",
    "    # wrap dataset into a dataloader\n",
    "    dl_train = torch.utils.data.DataLoader(dset_train, batch_size=b, shuffle=True)\n",
    "    \n",
    "    return dl_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1)  # no padding so we lose 2 pixels\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)  # no padding so we lose 2 pixels\n",
    "        self.dropout1 = nn.Dropout2d(p=0.25)\n",
    "        self.dropout2 = nn.Dropout2d(p=0.5)\n",
    "        self.fc1 = nn.Linear(in_features=64*12*12, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # input becomes 32x26x26\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)  # input becomes 64x24x24\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # input becomes 64x12x12\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)  # flatten into a single 9,216 dim vector\n",
    "        x = self.fc1(x)  # input goes from 9216 to 128\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)  # input goes from 128 to 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = fetch(7, 2)\n",
    "dataiter = iter(d)\n",
    "X_l, y_l = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(X_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2059035301208496\n",
      "tensor([[9],\n",
      "        [3]])\n",
      "2.214153528213501\n",
      "tensor([[9],\n",
      "        [9]])\n",
      "2.1456775665283203\n",
      "tensor([[7],\n",
      "        [7]])\n",
      "2.101571798324585\n",
      "tensor([[9],\n",
      "        [7]])\n",
      "2.101774215698242\n",
      "tensor([[7],\n",
      "        [7]])\n",
      "1.974748134613037\n",
      "tensor([[7],\n",
      "        [7]])\n",
      "1.986077070236206\n",
      "tensor([[7],\n",
      "        [7]])\n",
      "1.9112563133239746\n",
      "tensor([[7],\n",
      "        [7]])\n",
      "1.869182825088501\n",
      "tensor([[7],\n",
      "        [7]])\n",
      "1.7977864742279053\n",
      "tensor([[7],\n",
      "        [7]])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # step 1: train on labeled data\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_l)\n",
    "    loss = F.cross_entropy(output, y_l)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % (n_epochs / 10) == 0:\n",
    "        print(loss.item())\n",
    "        \n",
    "    # step 2: view probabilities on unlabeled data\n",
    "    X_u, y_u = dataiter.next()\n",
    "    output = model(X_u)\n",
    "    probs = F.softmax(output, dim=1)\n",
    "    pred = probs.argmax(dim=1, keepdim=True)\n",
    "    if i % (n_epochs / 10) == 0:\n",
    "        print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
